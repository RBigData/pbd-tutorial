\section{pbdR}
\makesubcontentsslides

\subsection{The pbdR Project}
\makesubcontentsslidessec

\begin{frame}{R Continues to Move Up in Popularity!}
\vspace{-2em}
  \begin{center}
    IEEE Spectrum's 2015 Ranking of Programming Languages\\[1ex]
    \includegraphics[scale=.45]{../common/pics/IEEE_Spectrum_Rank.png}
  \end{center}
  \vspace{-3em}
 {\scriptsize
\url{%
http://spectrum.ieee.org/computing/software/the-2015-top-ten-programming-languages
}}
\end{frame}

\begin{frame}{\pbdR Interfaces to Libraries: Sustainable Path}
  \vspace{-1ex}
  \centering\includegraphics[trim=0cm 5cm 0cm 3cm,clip=true,width=0.85\textwidth]
  {../common/pics/hardware/ParallelHardware27.pdf}
  \scriptsize
  \begin{block}{Why use HPC libraries?}
    \begin{itemize}[<+-|alert@+>]
    \item Many simulation science communities are invested in their API
    \item Data analysis uses much of the same basic math as simulation science
    \item The libraries represent 30+ years of parallel algorithm research
    \item \emph{They're tested.} \emph{They're
        fast.}  \emph{They're scalable.}
    \end{itemize}
  \end{block}
\end{frame}

\subsection{pbdMPI}
\makesubcontentsslidessec

\begin{frame}
  \begin{block}{pbdMPI: a High Level Interface to MPI}
    \begin{itemize}
    \item API is simplified: defaults in control objects.
    \item S4 methods: extensible to complex \R objects.
    \item Additional error checking
    \item Array and matrix methods without serialization: faster than
      \pkg{Rmpi}.
    \end{itemize}
    \begin{center}
      \vspace{0.2cm}\scriptsize
      \begin{tabular}{ll} \hline\hline
        \pkg{pbdMPI} (S4) & \pkg{Rmpi}                \\ \hline
        \code{\color{blue}allreduce}    & \code{mpi.allreduce}      \\
        \code{\color{blue}allgather}    & \code{mpi.allgather},
        \code{mpi.allgatherv},
        \code{mpi.allgather.Robj} \\
        \code{bcast}        & \code{mpi.bcast},
        \code{mpi.bcast.Robj}     \\
        \code{gather}       & \code{mpi.gather},
        \code{mpi.gatherv},
        \code{mpi.gather.Robj}    \\
        \code{recv}         & \code{mpi.recv},
        \code{mpi.recv.Robj}      \\
        \code{reduce}       & \code{mpi.reduce}         \\
        \code{scatter}      & \code{mpi.scatter},
        \code{mpi.scatterv},
        \code{mpi.scatter.Robj}   \\
        \code{send}         & \code{mpi.send},
        \code{mpi.send.Robj}      \\ \hline \hline
      \end{tabular}
    \end{center}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \begin{block}{Integer?\qquad Not always obvious in R.}
    \vspace{-.2cm}
    \begin{lstlisting}
> is.integer(1)
[1] FALSE
> is.integer(2)
[1] FALSE
> is.integer(1:2)
[1] TRUE
    \end{lstlisting}
  \end{block}
  \begin{block}{pbdMPI lets R figure it out}\pause
    \begin{minipage}[t]{.475\textwidth}
      \begin{lstlisting}[title=Rmpi]
# int
mpi.allreduce(x, type=1)
# double
mpi.allreduce(x, type=2)
      \end{lstlisting}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{.475\textwidth}
      \begin{lstlisting}[title=pbdMPI]
allreduce(x)
      \end{lstlisting}
      % \vspace{1em}
      % \hspace{1em}{\small S4. Batch only! (No spawning)}
    \end{minipage}
  \end{block}
\end{frame}


\begin{frame}[fragile]{Single Program (SPMD): Runs Asynchronous Parallel}
  \begin{exampleblock}{Rank Query Example}
    \centering
    \begin{lstlisting}[title=1\_rank.r]
library(pbdMPI, quiet = TRUE)
init()

my.rank <- comm.rank()
comm.print(my.rank, all.rank=TRUE)

finalize()
    \end{lstlisting}
    \begin{columns}[t,onlytextwidth]
      \begin{column}{0.62\textwidth}
        \begin{lstlisting}[backgroundcolor=\color{white},keywordstyle=\color{black},
title=Execute this batch script via:]
mpirun -np 2 Rscript 1_rank.r
        \end{lstlisting}
      \end{column}
      \hfill
      \begin{column}{0.35\textwidth}
        \begin{lstlisting}[title=Sample Output:]
COMM.RANK = 0
[1] 0
COMM.RANK = 1
[1] 1
        \end{lstlisting}
      \end{column}
    \end{columns}
  \end{exampleblock}
\end{frame}

\subsection{pbdDMAT}
\makesubcontentsslidessec

\begin{frame}{Mapping a Matrix to Processors}
  \begin{block}{Processor Grid Shapes}
    \begin{table}[ht]
      \centering
      % \begin{subfigure}[b]{0.23\textwidth}
      %   \centering
      %   $\left[\begin{tabular}{l}
      %       0 \\ 1 \\ 2 \\ 3 \\ 4 \\ 5
      %     \end{tabular}\right]^T$
      %   \caption{$1\times 6$}
      % \end{subfigure}
      \begin{subfigure}[b]{0.23\textwidth}
        \centering
        $\left[\begin{tabular}{llllll}
            0 & 1 & 2 & 3 & 4 & 5
          \end{tabular}\right]$
        \vspace{1.4cm}
        \caption{$1\times 6$}
      \end{subfigure}%\hspace{-1cm}
      \begin{subfigure}[b]{0.23\textwidth}
        \centering
        $\left[\begin{tabular}{lll}
            0 & 1 & 2\\
            3 & 4 & 5
          \end{tabular}\right]$
        \caption{$2\times 3$}
      \end{subfigure}%
      \begin{subfigure}[b]{0.23\textwidth}
        \centering
        $\left[\begin{tabular}{ll}
            0 & 1 \\
            2 & 3\\
            4 & 5
          \end{tabular}\right]$
        \caption{$3\times 2$}
      \end{subfigure}
      \begin{subfigure}[b]{0.23\textwidth}
        \centering
        $\left[\begin{tabular}{l}
            0 \\ 1 \\ 2 \\ 3 \\ 4 \\ 5
          \end{tabular}\right]$
        \caption{$6\times 1$}
      \end{subfigure}
      \caption{Processor Grid Shapes with 6 Processors}\label{fig:gridshapes}
    \end{table}
  \end{block}
\end{frame}


\subsection{pbdDMAT (and pbdBASE, pbdSLAP)}
\newcommand{\Sp}{\hspace{3pt}}
\begin{frame}{Distributed Matrix and Vector Operations \hfill pbdDMAT}
  \begin{block}{Powered by ScaLAPACK, PBLAS, and BLACS (MKL, SciLIB,
      or ACML)}
    \begin{itemize}
    \item Block-cyclic data layout for scalability and efficiency
    \item No change in R syntax
    \item High-level convenience for data layout redistributions
      \begin{itemize}
      \item Row-major data: read row-block then convert to block-cyclic
      \item Column-major data: read column-block then convert to block-cyclic
      \end{itemize}
    \end{itemize}
  \end{block}
  \begin{block}{Global and local views of block-cyclic on a 2 $\times$ 3
      processor grid}
\tiny
\begin{minipage}{4.4cm}
$
\left[
      \begin{array}{@{\Sp}l@{\Sp}l@{\Sp}|@{\Sp}l@{\Sp}l@{\Sp}|@{\Sp}l@{\Sp}l@{\Sp}|@{\Sp}l@{\Sp}l@{\Sp}|@{\Sp}l@{\Sp}}
      \color{g11}x_{11} & \color{g11}x_{12} & \color{g12}x_{13} & \color{g12}x_{14} & \color{g13}x_{15} & \color{g13}x_{16} & \color{g11}x_{17} & \color{g11}x_{18} & \color{g12}x_{19}\\
      \color{g11}x_{21} & \color{g11}x_{22} & \color{g12}x_{23} & \color{g12}x_{24} & \color{g13}x_{25} & \color{g13}x_{26} & \color{g11}x_{27} & \color{g11}x_{28} & \color{g12}x_{29}\\\hline
      \color{g21}x_{31} & \color{g21}x_{32} & \color{g22}x_{33} & \color{g22}x_{34} & \color{g23}x_{35} & \color{g23}x_{36} & \color{g21}x_{37} & \color{g21}x_{38} & \color{g22}x_{39}\\
      \color{g21}x_{41} & \color{g21}x_{42} & \color{g22}x_{43} & \color{g22}x_{44} & \color{g23}x_{45} & \color{g23}x_{46} & \color{g21}x_{47} & \color{g21}x_{48} & \color{g22}x_{49}\\\hline
      \color{g11}x_{51} & \color{g11}x_{52} & \color{g12}x_{53} & \color{g12}x_{54} & \color{g13}x_{55} & \color{g13}x_{56} & \color{g11}x_{57} & \color{g11}x_{58} & \color{g12}x_{59}\\
      \color{g11}x_{61} & \color{g11}x_{62} & \color{g12}x_{63} & \color{g12}x_{64} & \color{g13}x_{65} & \color{g13}x_{66} & \color{g11}x_{67} & \color{g11}x_{68} & \color{g12}x_{69}\\\hline
      \color{g21}x_{71} & \color{g21}x_{72} & \color{g22}x_{73} & \color{g22}x_{74} & \color{g23}x_{75} & \color{g23}x_{76} & \color{g21}x_{77} & \color{g21}x_{78} & \color{g22}x_{79}\\
      \color{g21}x_{81} & \color{g21}x_{82} & \color{g22}x_{83} & \color{g22}x_{84} & \color{g23}x_{85} & \color{g23}x_{86} & \color{g21}x_{87} & \color{g21}x_{88} & \color{g22}x_{89}\\\hline
      \color{g11}x_{91} & \color{g11}x_{92} & \color{g12}x_{93} & \color{g12}x_{94} & \color{g13}x_{95} & \color{g13}x_{96} & \color{g11}x_{97} & \color{g11}x_{98} & \color{g12}x_{99}\\
      \end{array}
\right]_{9\times 9}$
\end{minipage} \hspace{4.4em}
\begin{minipage}{6.5cm}
$\left[
      \begin{array}{@{\Sp}l@{\Sp}l@{\Sp}|@{\Sp}l@{\Sp}l@{\Sp}}
      \color{g11}x_{11} & \color{g11}x_{12} & \color{g11}x_{17} & \color{g11}x_{18}\\
      \color{g11}x_{21} & \color{g11}x_{22} & \color{g11}x_{27} & \color{g11}x_{28}\\\hline
      \color{g11}x_{51} & \color{g11}x_{52} & \color{g11}x_{57} & \color{g11}x_{58}\\
      \color{g11}x_{61} & \color{g11}x_{62} & \color{g11}x_{67} & \color{g11}x_{68}\\\hline
      \color{g11}x_{91} & \color{g11}x_{92} & \color{g11}x_{97} & \color{g11}x_{98}\\
      \end{array}
\right]_{5\times 4}$\hspace{-1.4ex}
$\left[
      \begin{array}{@{\Sp}l@{\Sp}l@{\Sp}|@{\Sp}l@{\Sp}}
      \color{g12}x_{13} & \color{g12}x_{14} & \color{g12}x_{19}\\
      \color{g12}x_{23} & \color{g12}x_{24} & \color{g12}x_{29}\\\hline
      \color{g12}x_{53} & \color{g12}x_{54} & \color{g12}x_{59}\\
      \color{g12}x_{63} & \color{g12}x_{64} & \color{g12}x_{69}\\\hline
      \color{g12}x_{93} & \color{g12}x_{94} & \color{g12}x_{99}\\
      \end{array}
\right]_{5\times 3}$ \hspace{-2ex}
$\left[
      \begin{array}{@{\Sp}l@{\Sp}l@{\Sp}}
      \color{g13}x_{15} & \color{g13}x_{16}\\
      \color{g13}x_{25} & \color{g13}x_{26}\\\hline
      \color{g13}x_{55} & \color{g13}x_{56}\\
      \color{g13}x_{65} & \color{g13}x_{66}\\\hline
      \color{g13}x_{95} & \color{g13}x_{96}\\
      \end{array}
\right]_{5\times 2}$
\\
$\left[
      \begin{array}{@{\Sp}l@{\Sp}l@{\Sp}|@{\Sp}l@{\Sp}l@{\Sp}}
      \color{g21}x_{31} & \color{g21}x_{32} & \color{g21}x_{37} & \color{g21}x_{38}\\
      \color{g21}x_{41} & \color{g21}x_{42} & \color{g21}x_{47} & \color{g21}x_{48}\\\hline
      \color{g21}x_{71} & \color{g21}x_{72} & \color{g21}x_{77} & \color{g21}x_{78}\\
      \color{g21}x_{81} & \color{g21}x_{82} & \color{g21}x_{87} & \color{g21}x_{88}\\
      \end{array}
\right]_{4\times 4}$ \hspace{-2ex}
$\left[
      \begin{array}{@{\Sp}l@{\Sp}l@{\Sp}|@{\Sp}l@{\Sp}}
      \color{g22}x_{33} & \color{g22}x_{34} & \color{g22}x_{39}\\
      \color{g22}x_{43} & \color{g22}x_{44} & \color{g22}x_{49}\\\hline
      \color{g22}x_{73} & \color{g22}x_{74} & \color{g22}x_{79}\\
      \color{g22}x_{83} & \color{g22}x_{84} & \color{g22}x_{89}\\
      \end{array}
\right]_{4\times 3}$ \hspace{-2ex}
$\left[
      \begin{array}{@{\Sp}l@{\Sp}l@{\Sp}}
      \color{g23}x_{35} & \color{g23}x_{36} \\
      \color{g23}x_{45} & \color{g23}x_{46} \\\hline
      \color{g23}x_{75} & \color{g23}x_{76} \\
      \color{g23}x_{85} & \color{g23}x_{86} \\
      \end{array}
\right]_{4\times 2}$
\end{minipage}
\end{block}
\end{frame}

\begin{frame}[fragile]
  \begin{block}{\pbdR\ No change in syntax \hfill + Data
      redistribution functions}
\vspace{-2ex}
  \begin{lstlisting}
x <- x[-1, 2:5]
x <- log(abs(x) + 1)
x.pca <- prcomp(x)
xtx <- t(x) %*% x
ans <- svd(solve(xtx))
  \end{lstlisting}
\vspace{-1ex}
  \begin{center}
  \emph{The above (and over 100 other functions) runs on 1 core with R \\
    or 10,000 cores with \pbdR ddmatrix class}
  \end{center}
\vspace{-2ex}
\begin{lstlisting}
> showClass("ddmatrix")
Class "ddmatrix" [package "pbdDMAT"]
Slots:
Name:     Data     dim    ldim   bldim   ICTXT
Class:  matrix numeric numeric numeric numeric
\end{lstlisting}
\vspace{-2ex}
\begin{lstlisting}
> x <- as.rowblock(x)
> x <- as.colblock(x)
> x <- redistribute(x, bldim=c(8, 8), ICTXT = 0)
\end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{pbdDMAT Scalability Benchmarks}
    \begin{itemize}[<+-|alert@+>]
      \item Default choices throughout (no MKL, ACML, etc.)
      \item 1 core = 1 MPI process (Kraken: 6-core Opterons)
      \item Generate random matrix
        \begin{itemize}
        \item Global Columns: 500, 1000, and 2000
        \item Global Rows: fixed per core to make $43.4 MiB$
        \end{itemize}
      \item Measure wall clock time
      \item ``weak scaling'' = global problem grows with core count
    \end{itemize}
    \vspace{.8cm}
    \centering\includegraphics{../common/pics/krakenWide}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \begin{block}{pbdDMAT Scalability Benchmarks}
    \begin{minipage}{0.44\textwidth}
      \vspace{-3ex}
      \begin{lstlisting}
x <- ddmatrix("rnorm", nrow=n, ncol=p)
cov.x <- cov(x)
      \end{lstlisting}
      \vspace{-3ex}
      \begin{center}
        \includegraphics[trim=3mm 1mm 2mm 11.5mm,clip,width=.98\textwidth]{../common/pics/cov}
      \end{center}
    \end{minipage}
%    \hspace{.01ex}
    \begin{minipage}{0.55\textwidth}
      \vspace{-3.5ex}
      \begin{lstlisting}
b <- ddmatrix("runif", nrow=p, ncol=1)
y <- x %*% b
b.hat <- lm.fit(x, y)$coefficients
      \end{lstlisting} %$
      \vspace{-3ex}
      \begin{center}
        \includegraphics[trim=3mm 1mm 2mm
        12mm,clip,width=.98\textwidth]{../common/pics/benchmarks/lmfit2}
      \end{center}
    \end{minipage}
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Matrix Exponentiation (pbdDMAT)}
    \begin{minipage}{5cm}
      \begin{itemize}\tiny
      \item Fitting biogeography models requires many matrix exponentiations
      \item Benchmark: Matrix exponential of 5000$\times$5000 matrix.
      \item R 3.1.0, Matrix 1.1-2, rexpokit 0.25, pbdDMAT 0.3-0
      \item Libs: Cray LibSci, NETLIB ScaLAPACK, Compilers: gnu 4.8.2
      \item Configuration: 1 thread == 1 MPI rank == 1 physical core
      \end{itemize}
      \vspace{-4ex}
      \begin{center}
        \includegraphics[trim=4cm 2cm 3.5cm 2.2cm,clip=true,height=4cm]{pics/Biogeography}
      \end{center}
    \end{minipage}
    \begin{minipage}{6.9cm}
      \includegraphics[trim=1mm 1mm 1mm 1mm,clip=true,height=7cm]{pics/MatExp}
    \end{minipage}
  \end{block}
  \begin{raggedright}\tiny
    Schmidt and Matzke (2014) Distributed matrix exponentiation, The R
    User Conference (UseR! 2014), \\[-2ex] Los Angeles, CA, August 2014 .
  \end{raggedright}
\end{frame}

\subsection{pbdDMAT Example: Truncated SVD from Random Projections}
\makesubcontentsslidessec
\input{../common/06_dmatstats/randsvd-slide}

\input{../common/04_stats/randforest.tex}

\input{../common/04_stats/fda.tex}


%\input{../common/06_dmatstats/decomprecomp.tex}


